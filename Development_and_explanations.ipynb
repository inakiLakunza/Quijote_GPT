{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9777a260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\ialak\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\ialak\\anaconda3\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=f0eecf865e74f26f69928bbc46988672a6a66434d5537b63ed855a39bbc40d59\n",
      "  Stored in directory: c:\\users\\ialak\\appdata\\local\\pip\\cache\\wheels\\04\\5f\\3e\\46cc37c5d698415694d83f607f833f83f0149e49b3af9d0f38\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "\n",
      "Saved under tiny_shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "# Download the tiny Shakespeare dataset to make\n",
    "# different trials with it in an easy way\n",
    "# !pip install wget\n",
    "# !python -m wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -o tiny_shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ea45c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tiny_shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f2ac60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1860c3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first 1000 characters:\n",
    "print(text[: 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a213f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Number of unique characters: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(''.join(chars))\n",
    "print(f\"Number of unique characters: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4b817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "962d1172",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "#### Convert the raw text as a string to some sequence of integers according to some vocabulary of possible elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da3c2033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 50, 1, 47, 52, 45, 43, 52, 47, 53, 57, 53, 1, 46, 47, 42, 39, 50, 45, 53, 1, 42, 53, 52, 1, 29, 59, 47, 48, 53, 58, 43, 1, 42, 43, 1, 50, 39, 1, 25, 39, 52, 41, 46, 39]\n",
      "El ingenioso hidalgo don Quijote de la Mancha\n"
     ]
    }
   ],
   "source": [
    "# Create a mapping from characters to integers\n",
    "\n",
    "# String to integer\n",
    "s2i = {ch:i for i,ch in enumerate(chars)}\n",
    "\n",
    "# Integer to string\n",
    "i2s = {i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encoder: take a string, output a list of integers\n",
    "encode = lambda s: [s2i[c] for c in s]\n",
    "\n",
    "# Decoder: Take a list of integers, output a string\n",
    "decode = lambda l: \"\".join([i2s[i] for i in l])\n",
    "\n",
    "example_phrase = \"El ingenioso hidalgo don Quijote de la Mancha\"\n",
    "print(encode(example_phrase))\n",
    "print(decode(encode(example_phrase)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b0d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01daef8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "\n",
      "\n",
      "First 1000 characters tokenized:\n",
      "\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Encode entire Tiny Shakespeare dataset and store it into a torch.Tensor\n",
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(\"\\n\\nFirst 1000 characters tokenized:\\n\")\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10273b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ecd8ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up the data into train and validation sets\n",
    "# 90% -> training, 10% validation\n",
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1165bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795bd2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fe83985",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We have to choose a *block size*, which will take consist of a series of tokens fed together into the model.\n",
    "\n",
    "When we sample a chunk of data this way, we have multiple samples packed into it, since all of the characters follow each other.\n",
    "\n",
    "So, we will train the transformer to make predictions *at every one of these positions*.\n",
    "\n",
    "If we select a block size of value 8, for instance, we will have a chunk of 9 characters, which will contain 8 individual examples packed in there.\n",
    "\n",
    "For instance, in the example above: in the context of [18], 47 comes next; in the context of [18, 47], 56 comes next, and so on. That is why we have to insert a +1 in order to have the chosen number of examples in a given block.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ab6ebaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33d47419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]), the target is: 47\n",
      "When input is tensor([18, 47]), the target is: 56\n",
      "When input is tensor([18, 47, 56]), the target is: 57\n",
      "When input is tensor([18, 47, 56, 57]), the target is: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]), the target is: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]), the target is: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context}, the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5b841b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]), the target is: 47\n",
      "When input is tensor([18, 47]), the target is: 56\n",
      "When input is tensor([18, 47, 56]), the target is: 57\n",
      "When input is tensor([18, 47, 56, 57]), the target is: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]), the target is: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]), the target is: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is: 58\n"
     ]
    }
   ],
   "source": [
    "# Same code as above, both do the same thing but explained\n",
    "# in different ways. The code above creates a y list which \n",
    "# 'is one step ahead of x', meaning that when we access an index\n",
    "# i in the x list, accessing the same index i in the list y will\n",
    "# give us the next element y the dataset\n",
    "\n",
    "# This code does the same, but using one list, ranging from the\n",
    "# beginning to the position at time t (including t, that is why [:t+1]),\n",
    "# and then we acces the element at index t+1\n",
    "\n",
    "x = train_data[:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = x[t+1]\n",
    "    print(f\"When input is {context}, the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fbcf1e",
   "metadata": {},
   "source": [
    "This way, the transformer will see all the contexts from as little as one to all the way to the chosen block size. And we would like the transformer to be used to seeing everything in between.\n",
    "\n",
    "And that is later gonna be useful when doing inference because *while we are sampling we can start the sampling generation* with as little as one character context. And the Transformer knows how to predict the next character with all the way up to just one context of one.\n",
    "\n",
    "So, it can predict everything up to block size and after block size we have to start truncating because the transformer will never receive more than block size when it is predicting characters.\n",
    "\n",
    "As we are sampling these chunks of text, every time we are going to fed them into a transformer we are going to have many batches of multiple chunks of text that are all stacked up in a single tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7102a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2de7e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "\n",
      "----\n",
      "When input is [24] the target is: 43\n",
      "When input is [24, 43] the target is: 58\n",
      "When input is [24, 43, 58] the target is: 5\n",
      "When input is [24, 43, 58, 5] the target is: 57\n",
      "When input is [24, 43, 58, 5, 57] the target is: 1\n",
      "When input is [24, 43, 58, 5, 57, 1] the target is: 46\n",
      "When input is [24, 43, 58, 5, 57, 1, 46] the target is: 43\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43] the target is: 39\n",
      "When input is [44] the target is: 53\n",
      "When input is [44, 53] the target is: 56\n",
      "When input is [44, 53, 56] the target is: 1\n",
      "When input is [44, 53, 56, 1] the target is: 58\n",
      "When input is [44, 53, 56, 1, 58] the target is: 46\n",
      "When input is [44, 53, 56, 1, 58, 46] the target is: 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39] the target is: 58\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58] the target is: 1\n",
      "When input is [52] the target is: 58\n",
      "When input is [52, 58] the target is: 1\n",
      "When input is [52, 58, 1] the target is: 58\n",
      "When input is [52, 58, 1, 58] the target is: 46\n",
      "When input is [52, 58, 1, 58, 46] the target is: 39\n",
      "When input is [52, 58, 1, 58, 46, 39] the target is: 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58] the target is: 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1] the target is: 46\n",
      "When input is [25] the target is: 17\n",
      "When input is [25, 17] the target is: 27\n",
      "When input is [25, 17, 27] the target is: 10\n",
      "When input is [25, 17, 27, 10] the target is: 0\n",
      "When input is [25, 17, 27, 10, 0] the target is: 21\n",
      "When input is [25, 17, 27, 10, 0, 21] the target is: 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1] the target is: 54\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54] the target is: 39\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('\\ntargets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('\\n----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When input is {context.tolist()} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8854ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3470c72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed221ee1",
   "metadata": {},
   "source": [
    "## Bigram Language Model\n",
    "\n",
    "A Bigram model is a language model in which we predict the probability of the correctness of a sequence of words by just predicting the occurrence of the word “a” after the word “b”.\n",
    "\n",
    "For eg: Let us take the sentence “I have a car”\n",
    "\n",
    "probability of the sequence of these words will be\n",
    "\n",
    "p(I) * p(have| I) * p(a | have, I) …….\n",
    "\n",
    "In the Bigram model, we consider that the above equation will give the same result as that of\n",
    "\n",
    "p(I) * p(have|I) * p(a|have) * p(car|a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "863416a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "Cross entropy loss: 4.878634929656982\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets):\n",
    "        \n",
    "        # WE ARE PREDICTING WHAT COMES NEXT JUST BY USING THE INDIVIDUAL\n",
    "        # IDENTITY OF A SINGLE TOKEN. We can do this since currently\n",
    "        # the token are not talking to each other, they are not seein\n",
    "        # any context except for that they are just seeing themselves\n",
    "        \n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
    "        \n",
    "        # The functional form of the cross entropy given by PyTorch\n",
    "        # expects the channel dimension C to be in the second position,\n",
    "        # it wants as input (B, C, T)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C) # We will stretch the Batch and Time dimensions\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(f\"Cross entropy loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc6791c",
   "metadata": {},
   "source": [
    "Our vocab size is 65, so the loss should be about -ln(1/65)=4.17 , but we are getting 4.879. So, what this is telling us is that *the initial predictions* are not diffused, they have a little bit of entropy and so we are guessing wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a1197dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "Cross entropy loss: 4.878634929656982\n"
     ]
    }
   ],
   "source": [
    "# Adding generation\n",
    "torch.manual_seed(1337)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else: \n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # logits becomes (B, C)\n",
    "            # apply Softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx  \n",
    "    \n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(f\"Cross entropy loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30f4c186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "# GENERATE FROM THE MODEL\n",
    "\n",
    "# creating a batch of just one element, and time will also be 1, \n",
    "# so, we have a (1, 1) tensor which contains a 0, of integer type (torch.long)\n",
    "# THIS 0 IS HOW WE WILL KICK OF THE GENERATION, BECAUSE AT INDEX 0 WE HAVE\n",
    "# THE *NEW LINE CHARACTER \\n*, so it is a reasonable character to begin\n",
    "# the generation\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "# We are going to generate 100 tokens, and since it creates it by\n",
    "# batches and we have just inserted one batch, we will have to extract\n",
    "# it using [0]. \n",
    "\n",
    "# And then that just gives us time steps, which is just a one-dimensional\n",
    "# array of all the indices which we will convert to simple python list \n",
    "# from our PyTorch tensor, so it can feed into our decode function and\n",
    "# so convert those integers into text\n",
    "\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3627dfc",
   "metadata": {},
   "source": [
    "The generate function we have just created does not make much sense since we are feeding in all this, we are building out this context and we are concatenating it all and we are always feeding it all into the model. *But right now we are just using a simple Bigram Model*, so we are not using the entire context we are feeding it with, we are just using the previous token.\n",
    "\n",
    "But later we will work with this function using the whole context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4d09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0a984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c40cee31",
   "metadata": {},
   "source": [
    "## Training the Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7f7815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "575e7207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4199717044830322\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2022cf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANGO:\n",
      "He rthay n thavee\n",
      "Sw s serer Fofow.\n",
      "Houspathe t:\n",
      "Mind fit.\n",
      "DUKINoceamy hun.\n",
      "CKIUShorst onre t ache bar, simed?\n",
      "And me theluse BHENurind-g'sto f w m CK:\n",
      "YCESI fatass mbre lious ave\n",
      "Wer'dor' wod y:\n",
      "\n",
      "Henkns ges wise we me y to elil'doug p in t her spalisusin t wndalu?Y!\n",
      "\n",
      "CKINENGLOFrkeang-lumod n odas ine a! thayayor hannd t; frat.\n",
      "OLArZAUSum,\n",
      "s I f pin hondecharvyouke helldid t we keicetlot llly ide ply t com: chavily; o fill wf mes co it hitodntly d d wsue ater to ovithA herisch ave une, t e pothamef. g st ofa nde hewhilero f tu dreanket iser oust he, s\n",
      "s t,end ald atant\n",
      "\n",
      "Turce dinsest; fowis MI ar gd yomblthaghrenenke this imy'dious wen histute ait MNow.\n",
      "Shid thod thokedanenj's.\n",
      "DUK: ishouthat,\n",
      "'s'd?\n",
      "S:'lENoran igoventitean he, tt usWhyour ixxff ou blvelife eare mie bu pamak.\n",
      "COnithololodaketwer thait g ESoco pe teran Gly? go!\n",
      "Slirenkneverajest young y, cet l th e?\n",
      "I he t ho jusofas my wd the y, ard h fes, her, KE: e,\n",
      "\n",
      "Heas tiluroullowh the as hy ea if ansountante dveaghed shavir-\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b95b6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
